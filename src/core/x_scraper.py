import os
import shutil
import asyncio
from datetime import datetime
from dotenv import load_dotenv
from playwright.async_api import async_playwright
import subprocess
import glob

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class XScraper:
    def __init__(self, username: str, time_range: str = "1ä¸ªæœˆ", download_root: str = "downloads", cookies_raw: str = None):
        """
        åˆå§‹åŒ– X å¹³å°çˆ¬è™«ã€‚
        :param username: éœ€è¦æŠ“å–çš„ X ç”¨æˆ·å
        :param time_range: æŠ“å–çš„æ—¶é—´èŒƒå›´ (å½“å¤©/3å¤©/1å‘¨/1ä¸ªæœˆ/1å¹´/å…¨éƒ¨)
        :param download_root: ä¸‹è½½æ–‡ä»¶çš„ä¸´æ—¶æ ¹ç›®å½•
        :param cookies_raw: X å¹³å°çš„åŸå§‹ Cookie å­—ç¬¦ä¸² (JSON æ ¼å¼)
        """
        self.username = username
        self.time_range = time_range
        self.download_root = download_root
        self.cookies_raw = cookies_raw
        self.user_download_dir = os.path.join(self.download_root, self.username)
        self.today_str = datetime.now().strftime("%Y-%m-%d")

    async def _load_cookies(self, context):
        import json
        if not self.cookies_raw:
            return
        try:
            cookies = json.loads(self.cookies_raw)
            valid_count = 0
            for cookie in cookies:
                try:
                    clean = {
                        'name': cookie.get('name', ''),
                        'value': cookie.get('value', ''),
                        'sameSite': 'Lax'
                    }
                    domain = cookie.get('domain', '')
                    if domain:
                        clean['domain'] = domain
                        clean['path'] = cookie.get('path', '/')
                    else:
                        continue
                    if cookie.get('secure'):
                        clean['secure'] = True
                    if cookie.get('httpOnly'):
                        clean['httpOnly'] = True
                    if cookie.get('expirationDate'):
                        clean['expires'] = cookie['expirationDate']
                    
                    await context.add_cookies([clean])
                    valid_count += 1
                except Exception:
                    continue
            print(f"âœ… X å¹³å°ï¼šåŠ è½½äº† {valid_count} ä¸ª Cookies")
        except Exception as e:
            print(f"âš ï¸ X å¹³å°ï¼šè§£æ Cookies å¤±è´¥: {e}")

    def _prepare_cookies_file(self) -> str:
        """ä¸º yt-dlp å‡†å¤‡ Netscape æ ¼å¼çš„ Cookie æ–‡ä»¶"""
        if not self.cookies_raw:
            return None
        cookie_file = "twitter_cookies.txt"
        import json
        try:
            data = json.loads(self.cookies_raw)
            if isinstance(data, list):
                with open(cookie_file, 'w') as f:
                    f.write("# Netscape HTTP Cookie File\n")
                    for c in data:
                        domain = c.get('domain', '')
                        flag = 'TRUE' if domain.startswith('.') else 'FALSE'
                        path = c.get('path', '/')
                        secure = 'TRUE' if c.get('secure') else 'FALSE'
                        expiration = str(int(c.get('expirationDate', 0)))
                        name = c.get('name', '')
                        value = c.get('value', '')
                        f.write(f"{domain}\t{flag}\t{path}\t{secure}\t{expiration}\t{name}\t{value}\n")
                return cookie_file
        except:
            pass
        return None

    async def scrape_tweet_urls(self, context) -> list:
        """åˆ©ç”¨ Playwright é¡µé¢æ»šåŠ¨æŠ“å–å¸¦æœ‰åª’ä½“çš„æ¨æ–‡é“¾æ¥ï¼ŒåŠ¨æ€åŸºäºæ—¶é—´èŒƒå›´"""
        from datetime import datetime, timedelta, timezone
        now_utc = datetime.now(timezone.utc)
        
        # ç¡®å®šæ—¶é—´é™åˆ¶ä¸æœ€å¤§æ»šåŠ¨æ¬¡æ•°
        if self.time_range == "å½“å¤©":
            time_limit = now_utc - timedelta(days=1)
            max_scrolls = 25
        elif self.time_range == "3å¤©":
            time_limit = now_utc - timedelta(days=3)
            max_scrolls = 40
        elif self.time_range == "1å‘¨":
            time_limit = now_utc - timedelta(days=7)
            max_scrolls = 60
        elif self.time_range == "1ä¸ªæœˆ":
            time_limit = now_utc - timedelta(days=30)
            max_scrolls = 150
        elif self.time_range == "1å¹´":
            time_limit = now_utc - timedelta(days=365)
            max_scrolls = 1000
        elif self.time_range == "å…¨éƒ¨":
            time_limit = datetime.min.replace(tzinfo=timezone.utc)
            max_scrolls = 3000
        else: # default 1 ä¸ªæœˆ
            time_limit = now_utc - timedelta(days=30)
            max_scrolls = 150
            self.time_range = "1ä¸ªæœˆ"

        page = await context.new_page()
        tweet_urls = set()
        print(f"ğŸ” æ­£åœ¨æ‰«æä¸»é¡µ (ç›®æ ‡èŒƒå›´: {self.time_range}): https://x.com/{self.username}")
        try:
            await page.goto(f"https://x.com/{self.username}", timeout=60000)
            try:
                 await page.wait_for_selector('article[data-testid="tweet"]', timeout=30000)
            except Exception as e:
                 current_url = page.url
                 title = await page.title()
                 print(f"âš ï¸ XScraper ç­‰å¾…æ¨æ–‡è¶…æ—¶ (å¯èƒ½æ˜¯ç™»å½•å¤±è´¥æˆ–è¯¥è´¦å·æ— æ–°å†…å®¹)")
                 print(f"ğŸ” è°ƒè¯•ä¿¡æ¯: å½“å‰ URL ä¸º {current_url} | é¡µé¢æ ‡é¢˜: {title}")
                 if "login" in current_url or "account/access" in current_url:
                     print("ğŸš¨ è‡´å‘½é”™è¯¯: æ‚¨çš„ X å¹³å°è®¿é—®è¢«é‡å®šå‘åˆ°äº†ç™»å½•é¡µæˆ–é”å®šé¡µï¼Œè¿™ä»£è¡¨æ‰€ä½¿ç”¨çš„ TWITTER_COOKIES å¿…ç„¶å·²å¤±æ•ˆï¼Œè¯·é‡æ–°æå–å¹¶é…ç½® Cookieã€‚")
                 elif "suspended" in current_url:
                     print("ğŸš¨ è‡´å‘½é”™è¯¯: è¯¥æ¨ç‰¹è´¦å·å·²è¢«å°ç¦ (Suspended)ã€‚")
                 return []

            reached_time_limit = False

            for i in range(max_scrolls):
                if reached_time_limit:
                    print(f"  â³ å·²æŠ“å–åˆ° {self.time_range} å‰çš„æ•°æ®ï¼Œåœæ­¢å‘ä¸‹æ»šåŠ¨ã€‚")
                    break

                articles = await page.locator('article[data-testid="tweet"]').all()
                for article in articles:
                    try:
                        # æ£€æŸ¥æ¨æ–‡æ—¶é—´
                        time_loc = article.locator('time').first
                        if await time_loc.count() > 0:
                            date_str = await time_loc.get_attribute('datetime')
                            if date_str:
                                # date_str æ ¼å¼å¦‚ '2025-02-15T12:00:00.000Z'
                                tweet_date = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                                if tweet_date < time_limit:
                                    reached_time_limit = True

                        has_media = False
                        if await article.locator('div[data-testid="tweetPhoto"]').count() > 0:
                            has_media = True
                        elif await article.locator('div[data-testid="videoPlayer"]').count() > 0:
                            has_media = True
                            
                        if has_media:
                            link_loc = article.locator('a[href*="/status/"]').first
                            if await link_loc.count() > 0:
                                href = await link_loc.get_attribute("href")
                                full_url = f"https://x.com{href}".split("?")[0]
                                tweet_urls.add(full_url)
                    except:
                        continue
                
                # å‘ä¸‹æ»šåŠ¨ 2500 åƒç´ ï¼Œç­‰å¾…æ–°å†…å®¹åŠ è½½
                await page.evaluate("window.scrollBy(0, 2500)")
                # ç»™ç½‘é¡µä¸€ç‚¹æ—¶é—´æ¸²æŸ“åé¢çš„æ¨æ–‡
                await asyncio.sleep(2.5)
                
                if i > 0 and i % 10 == 0:
                    print(f"  ... å·²æ»šåŠ¨ {i} æ¬¡ï¼Œç›®å‰é‡‡é›†åˆ° {len(tweet_urls)} ä¸ªåª’ä½“æ¨æ–‡ã€‚")

        except Exception as e:
            print(f"âš ï¸ æŠ“å– {self.username} é¡µé¢å¼‚å¸¸: {e}")
        finally:
            await page.close()
            
        return list(tweet_urls)

    async def fetch_media_files(self) -> list:
        """
        å®Œæ•´å·¥ä½œæµï¼šä¸‹è½½è¯¥ç”¨æˆ·çš„æ‰€æœ‰åª’ä½“åˆ°æœ¬åœ°
        è¿”å›æœ¬åœ°ä¸‹è½½å¥½çš„æ–‡ä»¶ç»å¯¹è·¯å¾„åˆ—è¡¨ã€‚
        """
        # 1. å‡†å¤‡æœ¬åœ°ç›®å½•
        if os.path.exists(self.user_download_dir):
            shutil.rmtree(self.user_download_dir)
        os.makedirs(self.user_download_dir, exist_ok=True)
        
        cookie_file = self._prepare_cookies_file()

        # 2. æŠ“å– URLs
        async with async_playwright() as p:
            launch_args = ['--disable-blink-features=AutomationControlled', '--no-sandbox']
            browser = await p.chromium.launch(headless=True, args=launch_args)
            context = await browser.new_context(viewport={'width': 1920, 'height': 1080})
            await self._load_cookies(context)
            
            tweet_urls = await self.scrape_tweet_urls(context)
            await browser.close()

        if not tweet_urls:
            print(f"ğŸ“­ ç”¨æˆ· {self.username} æœ€è¿‘æ²¡æœ‰å¸¦åª’ä½“çš„æ¨æ–‡ã€‚")
            return []

        print(f"ğŸ“¥ å‘ç° {len(tweet_urls)} æ¡å¸¦æœ‰åª’ä½“çš„æ¨æ–‡ï¼Œå¼€å§‹ä¸‹è½½...")

        # 3. ä½¿ç”¨ gallery-dl æ›¿ä»£ yt-dlp æ‰§è¡Œä¸‹è½½
        for url in tweet_urls:
            cmd = [
                "gallery-dl",
                url,
                "--directory", self.user_download_dir,
            ]
            if cookie_file:
                 cmd.extend(["--cookies", cookie_file])
            
            subprocess.run(cmd, check=False)

        # 4. æ”¶é›†ä¸‹è½½çš„æ–‡ä»¶ (ç”±äº gallery-dl å¯èƒ½ä¼šç”Ÿæˆå¤šçº§å­æ–‡ä»¶å¤¹ï¼Œä¾‹å¦‚ twitter/ç”¨æˆ·å/å›¾ç‰‡.jpg)
        all_files = glob.glob(f"{self.user_download_dir}/**/*", recursive=True)
        files = [f for f in all_files if os.path.isfile(f)]
        
        if not files:
            print(f"âš ï¸ [{self.username}] ä¸‹è½½ç®¡çº¿ç»“æŸï¼Œä½†æ²¡æœ‰æŠ“åˆ°æ–‡ä»¶ã€‚")
            return []

        return files

    def cleanup(self):
        """æ¸…ç†ä¸´æ—¶ç”Ÿæˆçš„ä¸‹è½½ç›®å½•"""
        if os.path.exists(self.user_download_dir):
            shutil.rmtree(self.user_download_dir, ignore_errors=True)
        print(f"ğŸ—‘ï¸ å·²æ¸…ç† {self.username} çš„ä¸´æ—¶æ–‡ä»¶ã€‚")
