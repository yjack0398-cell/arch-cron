import os
import sys
import asyncio
import argparse

# å°† src ç›®å½•æ·»åŠ åˆ° sys.pathï¼Œç¡®ä¿å¯ä»¥å¯¼å…¥ core å’Œ uploaders
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from dotenv import load_dotenv
from core.x_scraper import XScraper
from uploaders.uploader_115 import Uploader115

load_dotenv()

async def main():
    parser = argparse.ArgumentParser(description="X å¹³å°æŠ“å–å¹¶ä¸Šä¼ è‡³ 115ç½‘ç›˜ å·¥ä½œæµ")
    parser.add_argument('--users', type=str, required=True, help="é€—å·åˆ†éš”çš„ X ç”¨æˆ·ååˆ—è¡¨")
    args = parser.parse_args()
    
    users = [u.strip() for u in args.users.split(',') if u.strip()]
    cookies_x = os.getenv("TWITTER_COOKIES")
    cookies_115 = os.getenv("COOKIES_115")
    
    if not cookies_115:
        print("âš ï¸ æœªé…ç½® 115 ç½‘ç›˜ COOKIESï¼Œæ— æ³•ä¸Šä¼ ï¼")
        return

    uploader = Uploader115(cookies_raw=cookies_115)
    
    from datetime import datetime
    today_str = datetime.now().strftime("%Y-%m-%d")

    for user in users:
        print(f"\nğŸš€ å¼€å§‹å¤„ç† [115ç½‘ç›˜] å¤‡ä»½ä»»åŠ¡: {user}")
        scraper = XScraper(username=user, cookies_raw=cookies_x)
        files = await scraper.fetch_media_files()
        
        if files:
            uploader.upload_files(
                files=files,
                remote_root="Twitter_Archive",
                user_name=user,
                today_str=today_str
            )
        scraper.cleanup()

if __name__ == "__main__":
    asyncio.run(main())
