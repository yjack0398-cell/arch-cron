"""
X å¹³å°æŠ“å–å¹¶ä¸Šä¼ è‡³å¤¸å…‹ç½‘ç›˜ å·¥ä½œæµ

ä½¿ç”¨ Playwright æµè§ˆå™¨æ¨¡æ‹Ÿæ–¹å¼ä¸Šä¼ æ–‡ä»¶åˆ°å¤¸å…‹ç½‘ç›˜
"""

import os
import sys
import asyncio
import argparse
import json

# å°† src ç›®å½•æ·»åŠ åˆ° sys.pathï¼Œè§£å†³ç›´æ¥è¿è¡Œæ—¶çš„ ModuleNotFoundError
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from dotenv import load_dotenv
from core.x_scraper import XScraper
from uploaders.uploader_quark import UploaderQuark

load_dotenv()

async def main():
    parser = argparse.ArgumentParser(description="X å¹³å°æŠ“å–å¹¶ä¸Šä¼ è‡³ å¤¸å…‹ç½‘ç›˜ å·¥ä½œæµ")
    parser.add_argument('--users', type=str, required=True, help="é€—å·åˆ†éš”çš„ X ç”¨æˆ·ååˆ—è¡¨")
    parser.add_argument('--time_range', type=str, default="1ä¸ªæœˆ", help="æŠ“å–æ—¶é—´èŒƒå›´ (å½“å¤©/3å¤©/1å‘¨/1ä¸ªæœˆ/1å¹´/å…¨éƒ¨)")
    args = parser.parse_args()
    
    users = [u.strip() for u in args.users.split(',') if u.strip()]
    cookies_x = os.getenv("TWITTER_COOKIES")
    cookies_quark = os.getenv("COOKIES_QUARK")
    
    if not cookies_quark:
        print("âš ï¸ æœªé…ç½® å¤¸å…‹ç½‘ç›˜ COOKIESï¼Œæ— æ³•ä¸Šä¼ ï¼")
        return

    from playwright.async_api import async_playwright
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            args=[
                '--disable-blink-features=AutomationControlled',
                '--no-sandbox',
                '--disable-dev-shm-usage',
            ]
        )
        context = await browser.new_context(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            viewport={'width': 1920, 'height': 1080},
        )
        
        # åŠ è½½å¤¸å…‹ Cookie
        try:
            cookies = json.loads(cookies_quark)
            clean_list = []
            for c in cookies:
                domain = c.get('domain', '')
                if domain.endswith('quark.cn'):
                    domain = '.quark.cn'
                clean = {
                    'name': c.get('name', ''),
                    'value': c.get('value', ''),
                    'domain': domain,
                    'path': c.get('path', '/'),
                }
                clean_list.append(clean)
            await context.add_cookies(clean_list)
            print(f"âœ… å·²åŠ è½½ {len(clean_list)} æ¡å¤¸å…‹ Cookie")
        except Exception as e:
            print(f"âš ï¸ è§£æå¤¸å…‹ Cookies å¤±è´¥: {e}")
            await browser.close()
            return

        # åˆ›å»ºä¸Šä¼ å™¨ï¼ˆPlaywright æµè§ˆå™¨æ¨¡æ‹Ÿæ–¹å¼ï¼‰
        uploader = UploaderQuark(cookies_raw=cookies_quark, browser_context=context)
        
        for user in users:
            print(f"\nğŸš€ å¼€å§‹å¤„ç† [å¤¸å…‹ç½‘ç›˜] å¤‡ä»½ä»»åŠ¡: {user} | èŒƒå›´: {args.time_range}")
            scraper = XScraper(username=user, time_range=args.time_range, cookies_raw=cookies_x)
            files = await scraper.fetch_media_files()
            
            if files:
                await uploader.upload_files(files=files, remote_root="Twitter_Archive")
            else:
                print(f"  â„¹ï¸ {user}: æ²¡æœ‰å‘ç°æ–°çš„åª’ä½“æ–‡ä»¶")
            scraper.cleanup()
        
        await browser.close()

if __name__ == "__main__":
    asyncio.run(main())
